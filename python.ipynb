{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting IRA_tweets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile IRA_tweets.py\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class IRA_tweets:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.intersect = ['hashtags', 'is_retweet', 'quote_count', 'reply_count', 'retweet_count']\n",
    "        \n",
    "        # load column names from ira data and matched column names from rtweet data\n",
    "        self.ira_cols = ['account_creation_date', 'account_language', 'follower_count',\n",
    "           'in_reply_to_tweetid', 'in_reply_to_userid',\n",
    "           'quoted_tweet_tweetid', 'retweet_tweetid', 'retweet_userid',\n",
    "           'tweet_language', 'tweet_text', 'tweet_time',\n",
    "           'tweetid', 'urls', 'user_display_name', 'user_mentions',\n",
    "           'user_profile_description', 'user_profile_url',\n",
    "           'user_reported_location', 'user_screen_name', 'userid']\n",
    "\n",
    "        self.rtweet_cols=['account_created_at', 'account_lang','followers_count','reply_to_status_id', 'reply_to_user_id',\n",
    "            'quoted_status_id', 'retweet_status_id', 'retweet_user_id','lang','text','created_at','status_id',\n",
    "            'urls_expanded_url','name','mentions_user_id','description','profile_expanded_url','location',\n",
    "            'screen_name','user_id']\n",
    "        \n",
    "        # full\n",
    "        self.rtweet_cols_fullset = pd.read_csv('samplehl.csv',nrows=1).columns\n",
    "        \n",
    "        self.data = pd.read_csv('ira_tweets_csv_hashed.csv',\n",
    "                usecols=self.intersect+self.ira_cols)#, nrows=100000)\n",
    "\n",
    "        #if 'tweet_time' in columns_of_interest: #convert to datetime format, include only date (not time)\n",
    "        self.data['tweet_time'] = pd.to_datetime(self.data['tweet_time'])\n",
    "        \n",
    "    def english_only_tweets(self):\n",
    "        def filter_out_nonenglish_tweets(df):\n",
    "            df=df[(df['account_language'] == 'en') & (df['tweet_language'] == 'en')]\n",
    "            return df\n",
    "\n",
    "        self.data = (self.data.pipe(filter_out_nonenglish_tweets).\n",
    "            drop(['account_language','tweet_language'],1))\n",
    "        \n",
    "    def round_dates_to_weekstarting(self):\n",
    "        def round_dates_to_week_starting_function(df):\n",
    "            df['tweet_time'] = df['tweet_time'] - pd.Timedelta('1d') * df['tweet_time'].dt.dayofweek\n",
    "            df['tweet_time'] = pd.to_datetime(df['tweet_time'].dt.date)\n",
    "            return df\n",
    "\n",
    "        self.data = self.data.pipe(round_dates_to_week_starting_function)\n",
    "        \n",
    "        \n",
    "    def get_number_of_weeks_account_posted(self):\n",
    "        self.number_of_weeks_account_posted = (self.data.groupby(['user_screen_name'],as_index=False).\n",
    "             agg({'tweet_time':'nunique'}).\n",
    "            sort_values('tweet_time',ascending=False))\n",
    "\n",
    "        return self.number_of_weeks_account_posted\n",
    "    \n",
    "    \n",
    "    def tweet_counts_by_account_and_week(self,top_IRA_accounts):\n",
    "        def get_tweet_counts(df):\n",
    "            return (df[df['user_screen_name'].isin(top_IRA_accounts)].\n",
    "                    groupby(['tweet_time','user_screen_name'],as_index=False).\n",
    "                     size().reset_index().sort_values(['user_screen_name','tweet_time']))\n",
    "\n",
    "        self.tweet_counts = self.data.pipe(get_tweet_counts)\n",
    "\n",
    "        return self.tweet_counts\n",
    "    \n",
    "    def save_data_for_bot_test(self,ta):\n",
    "        def reformat_account_record_rtweet(df,ta):\n",
    "            df=df[df['user_screen_name'] == ta]\n",
    "            new_columns = df.columns.values\n",
    "            for n,o in zip(self.rtweet_cols, self.ira_cols): new_columns[new_columns==o] = n\n",
    "            df.columns=new_columns\n",
    "\n",
    "            df=df.loc[:,self.rtweet_cols_fullset]\n",
    "            return df\n",
    "\n",
    "        directory = 'bot_detection/'\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "        self.data.pipe(reformat_account_record_rtweet,ta).to_csv(directory+ta)\n",
    "        \n",
    "    def get_top_accounts_with_botornot_score(self,fn,top_IRA_accounts):\n",
    "        ## load probability each account is a bot\n",
    "        botornot = pd.read_csv(fn,index_col=0).loc[top_IRA_accounts]\n",
    "        top_IRA_accounts = botornot[~botornot.isnull().all(1)].index\n",
    "\n",
    "        return top_IRA_accounts\n",
    "\n",
    "    def create_wide_dataframe(self,tweet_counts):\n",
    "    \n",
    "        #create wide dataframe\n",
    "        def widen_and_impute(df):\n",
    "            df=df.pivot(index='tweet_time',columns='user_screen_name',values=0)\n",
    "\n",
    "            #range of dates at weekly intervals\n",
    "            periods =  ((df.index.max() - df.index.min())/7).days + 1\n",
    "            idx = pd.date_range(df.index.min(), df.index.max(),periods =periods)\n",
    "\n",
    "            #add missing weeks and replace missing values with zeros\n",
    "            df=df.reindex(idx).fillna(0)\n",
    "\n",
    "            return df.T\n",
    "\n",
    "        return tweet_counts.pipe(widen_and_impute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run_IRA_tweets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_IRA_tweets.py\n",
    "import IRA_tweets\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "self = IRA_tweets.IRA_tweets()\n",
    "self.english_only_tweets()\n",
    "self.round_dates_to_weekstarting()\n",
    "\n",
    "number_of_weeks_posted = self.get_number_of_weeks_account_posted()\n",
    "\n",
    "#accounts that tweeted in 100 or more weeks\n",
    "top_IRA_accounts = (number_of_weeks_posted[number_of_weeks_posted['tweet_time'] > 100]\n",
    "                    ['user_screen_name'].values)\n",
    "\n",
    "#get non-anonymized accountsgit br (short names)\n",
    "top_IRA_accounts = [w for w in top_IRA_accounts if len(w) < 20] \n",
    "\n",
    "# save log for high volume Twitter accounts in rtweet format\n",
    "#for ta in top_IRA_accounts:#[1]\n",
    "#    self.save_data_for_bot_test(ta)\n",
    "\n",
    "fn = 'botornot.csv' # path to botornot probabilities\n",
    "top_IRA_accounts = self.get_top_accounts_with_botornot_score(fn,top_IRA_accounts)\n",
    "\n",
    "tweet_counts = self.tweet_counts_by_account_and_week(top_IRA_accounts)\n",
    "df_ts = self.create_wide_dataframe(tweet_counts)\n",
    "\n",
    "df_ts.to_csv('IRA_timeseries')\n",
    "\n",
    "\n",
    "\n",
    "### plot and save rough pdfs of time series \n",
    "for name in top_IRA_accounts:\n",
    "\n",
    "    x=tweet_counts[tweet_counts['user_screen_name'] == name]\n",
    "    x.index=x['tweet_time']\n",
    "    x=x[0]\n",
    "\n",
    "    f,ax = plt.subplots()\n",
    "    \n",
    "    ax.plot(x,'o-')\n",
    "    ax.set_title(name)\n",
    "    \n",
    "    plot_directory = 'timeseries_plots/'\n",
    "    if not os.path.exists(plot_directory):\n",
    "        os.makedirs(plot_directory)\n",
    "        \n",
    "    plt.savefig(plot_directory + name + '.pdf')\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jenn_Abrams',\n",
       " 'DanaGeezus',\n",
       " 'KansasDailyNews',\n",
       " 'ChrixMorgan',\n",
       " 'GiselleEvns',\n",
       " 'DailySanJose',\n",
       " 'DailySanFran',\n",
       " 'DailySanDiego',\n",
       " 'PhoenixDailyNew',\n",
       " 'Seattle_Post',\n",
       " 'SanAntoTopNews',\n",
       " 'LoraGreeen',\n",
       " 'NewOrleansON',\n",
       " 'AmandaVGreen',\n",
       " 'IlikeBIGbuttand',\n",
       " 'NotRitaHart',\n",
       " 'WashingtOnline',\n",
       " 'ChicagoDailyNew',\n",
       " 'OnlineCleveland',\n",
       " 'HoustonTopNews',\n",
       " 'TodayPittsburgh',\n",
       " 'todayinsyria',\n",
       " 'gloed_up',\n",
       " 'TodayNYCity',\n",
       " 'DetroitDailyNew',\n",
       " 'OaklandOnline',\n",
       " 'StLouisOnline',\n",
       " 'BleepThePolice']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_IRA_accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DailySanFran', 'Jenn_Abrams', 'KansasDailyNews', 'SanAntoTopNews',\n",
       "       'Seattle_Post'], dtype=object)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_counts['user_screen_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
